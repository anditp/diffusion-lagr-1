#!/bin/bash 

# This is a job script, which was used for training the ScIDiff model

#SBATCH --partition=titanic
#SBATCH -t 00:01:00
#SBATCH --gres=gpu:4
#SBATCH --job-name=test_diff_lagr
#SBATCH --cpus-per-task=8
#SBATCH --output=logs/slurm_out_%j.stdout
#SBATCH --error=logs/slurm_out_%j.stderr
#SBATCH --mem=16g
#SBATCH --open-mode=append
#SBATCH --requeue
#SBATCH --requeue


DATESTAMP=`date +%Y%m%d`
# paths to store Slurm's log files
LOGS_FOLDER="${HOME}/local_logs/test_diff_lagr/${DATESTAMP}/${SLURM_JOB_ID}"
LOG_STDOUT="${HOME}/local_logs/test_diff_lagr/slurm_out_$SLURM_JOB_ID.stdout"

# Create logs folder
mkdir -p $LOGS_FOLDER

# ScIDiff-related arguments
TEST_PATH="$HOME/diffusion-lagr-1" # path to the ScIDiff repository
DATA_PATH="$HOME/diffusion-lagr-1/datasets/Lagr_u1c_diffusion-demo.h5" # path to the training data


# Start or restart experiment
date >> $LOG_STDOUT
echo "===== Starting job =====">> $LOG_STDOUT
echo "JOB ID: $SLURM_JOB_ID" >> $LOG_STDOUT

# Load the miniconda environment
source $HOME/.bashrc
conda init
conda activate

which python >> $LOG_STDOUT

# Training scidiff model with 3-D lagrangian trajectories
# MODELS_PATH can be a path to a directory that contains a weights.pt checkpoint file
python $TEST_PATH/turb_train.py --dataset_path $DATA_PATH --dataset_name train --dims 1 >> $LOG_STDOUT

# Move Slurm's logs to folder named with date
mv $HOME/$LOG_STDOUT $LOGS_FOLDER
mv $HOME/local_logs/test_diff_lagr/slurm_out_$SLURM_JOB_ID.stderr $LOGS_FOLDER
