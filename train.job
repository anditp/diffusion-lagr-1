#!/bin/bash 

# This is a job script, which was used for training the ScIDiff model

#SBATCH --partition=titanic
#SBATCH --gres=gpu:3
#SBATCH -t 24-00:00:00
#SBATCH --job-name=train_diff_lagr
#SBATCH --cpus-per-task=10
#SBATCH --output=logs/slurm_out.stdout
#SBATCH --error=logs/slurm_out.stderr
#SBATCH --mem=60g
#SBATCH --open-mode=append
#SBATCH --requeue
#SBATCH --requeue


DATESTAMP=`date +%Y%m%d`
# paths to store Slurm's log files
LOGS_FOLDER="${HOME}/local_logs/test_diff_lagr/${DATESTAMP}/${SLURM_JOB_ID}"
LOG_STDOUT="${HOME}/local_logs/test_diff_lagr/slurm_out_$SLURM_JOB_ID.stdout"


# Create logs folder
mkdir -p $LOGS_FOLDER

# ScIDiff-related arguments
TEST_PATH="$HOME/diffusion-lagr-1" # path to the ScIDiff repository

echo $CUDA_VISIBLE_DEVICES >> $LOG_STDOUT
echo $LOGS_FOLDER >> $LOG_STDOUT
DATA_FLAGS="--dataset_path /home/tau/apantea/data/velocities_paired.npy"
MODEL_FLAGS="--dims 1 --image_size 2000 --in_channels 2 --num_channels 128 --num_res_blocks 1 --channel_mult 1,1,2,3,4"
DIFFUSION_FLAGS="--diffusion_steps 400 --noise_schedule tanh6,1"
TRAIN_FLAGS="--lr 1e-5 --batch_size 96"


# Start or restart experiment
date >> $LOG_STDOUT
echo "===== Starting job =====">> $LOG_STDOUT
echo "JOB ID: $SLURM_JOB_ID" >> $LOG_STDOUT

# Load the miniconda environment
source $HOME/.bashrc

which python >> $LOG_STDOUT

# Training scidiff model with 3-D lagrangian trajectories
# MODELS_PATH can be a path to a directory that contains a weights.pt checkpoint file
echo "here" >> $LOG_STDOUT
python $TEST_PATH/turb_train.py $DATA_FLAGS $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS >> $LOG_STDOUT

# Move Slurm's logs to folder named with date
mv $HOME/$LOG_STDOUT $LOGS_FOLDER
mv $HOME/local_logs/test_diff_lagr/slurm_out_$SLURM_JOB_ID.stderr $LOGS_FOLDER
