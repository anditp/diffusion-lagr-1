#!/bin/bash 

# This is a job script, which was used for training the ScIDiff model

#SBATCH --partition=titanic
#SBATCH --gres=gpu:1
#SBATCH --nodelist titanic-2
#SBATCH -t 04-00:00:00
#SBATCH --job-name=sample_diff_lagr
#SBATCH --cpus-per-task=10
#SBATCH --output=logs/slurm_out.stdout
#SBATCH --error=logs/slurm_out.stderr
#SBATCH --mem=40g
#SBATCH --open-mode=append
#SBATCH --requeue
#SBATCH --requeue


DATESTAMP=`date +%Y%m%d`
# paths to store Slurm's log files
LOGS_FOLDER="${HOME}/local_logs/test_diff_lagr/${DATESTAMP}/${SLURM_JOB_ID}"
LOG_STDOUT="${HOME}/local_logs/test_diff_lagr/slurm_out_$SLURM_JOB_ID.stdout"


# Create logs folder
mkdir -p $LOGS_FOLDER

# ScIDiff-related arguments
TEST_PATH="$HOME/diffusion-lagr-1" # path to the ScIDiff repository

echo $LOGS_FOLDER >> $LOG_STDOUT
MODEL_FLAGS="--dims 1 --image_size 2000 --in_channels 1 --num_channels 32 --num_res_blocks 1 --channel_mult 1,2,4"
DIFFUSION_FLAGS="--diffusion_steps 800 --noise_schedule tanh6,1"
SAMPLE_FLAGS="--num_samples 10000 --batch_size 192 --model_path logs/ema_0.9999_250000.pt --output /home/tau/apantea/diffusion-lagr-1/logs/samples_${SLURM_JOB_ID}_1d.npy"


# Start or restart experiment
date >> $LOG_STDOUT
echo "===== Starting job =====">> $LOG_STDOUT
echo "JOB ID: $SLURM_JOB_ID" >> $LOG_STDOUT

# Load the miniconda environment
source $HOME/.bashrc

which python >> $LOG_STDOUT

# Training scidiff model with 3-D lagrangian trajectories
# MODELS_PATH can be a path to a directory that contains a weights.pt checkpoint file
echo "here" >> $LOG_STDOUT
python $TEST_PATH/turb_sample.py $SAMPLE_FLAGS $MODEL_FLAGS $DIFFUSION_FLAGS >> $LOG_STDOUT

# Move Slurm's logs to folder named with date
# mv $LOG_STDOUT $LOGS_FOLDER
# mv $HOME/local_logs/test_diff_lagr/slurm_out_$SLURM_JOB_ID.stderr $LOGS_FOLDER
